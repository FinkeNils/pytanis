{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import math\n",
    "import logging\n",
    "import structlog\n",
    "from pathlib import Path\n",
    "import json\n",
    "\n",
    "import tomli\n",
    "import numpy as np\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "\n",
    "import seaborn as sns\n",
    "sns.set_context(\"poster\")\n",
    "sns.set(rc={\"figure.figsize\": (12, 6.)})\n",
    "sns.set_style(\"whitegrid\")\n",
    "\n",
    "import pandas as pd\n",
    "pd.set_option(\"display.max_rows\", 120)\n",
    "pd.set_option(\"display.max_columns\", 120)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "logging.basicConfig(level=logging.WARNING, stream=sys.stdout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pytanis\n",
    "from pytanis import GSheetClient, PretalxClient\n",
    "from pytanis.pretalx import subs_as_df\n",
    "from pytanis.review import read_assignment_as_df, save_assignments_as_json, Col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0.5.post1.dev0+g481d83f.d20230409'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Be aware that this notebook might only run with the following version\n",
    "pytanis.__version__ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Import event-specific settings to don't have them here in the notebook\n",
    "with open('config.toml', 'rb') as fh:\n",
    "    cfg = tomli.load(fh)\n",
    "\n",
    "TARGET_REVIEWS = 3  # We want at least 3 reviews per proposal\n",
    "RND_STATE = 1729 # Random state or None for really random\n",
    "community_map = \"General: Community, Diversity, Career, Life and everything else\", \"General: Community\" # to make splitting easier in GSheet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read Reviews and all Submissions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9fc8aec2a79140ca861ba8ec4d8d564b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/427 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c5ca0efe8dfa4581af44df4c0f21d3a4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1390 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pretalx_client = PretalxClient(blocking=True)\n",
    "subs_count, subs = pretalx_client.submissions(cfg['event_name'])\n",
    "revs_count, revs = pretalx_client.reviews(cfg['event_name'])\n",
    "subs, revs = list(subs), list(revs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# count reviews that actually have scores\n",
    "revs_vcounts = pd.Series([r.submission for r in revs if r.score is not None]).value_counts()\n",
    "revs_vcounts = revs_vcounts.reset_index().rename(columns={\"index\": Col.submission, 0: Col.nreviews})\n",
    "# keep track of all reviews, i.e. proposals people interacted with\n",
    "revs_df_raw = pd.DataFrame([{\"created\": r.created, \"updated\": r.updated, Col.pretalx_user: r.user, \"score\": r.score, \"review\": r.submission} for r in revs])\n",
    "revs_df = revs_df_raw.groupby([Col.pretalx_user]).agg(lambda x: x.tolist()).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "subs_df = subs_as_df([sub for sub in subs if sub.state.value == \"confirmed\"])  # Take only submitted ones\n",
    "subs_df[Col.track].replace(dfict([community_map]), inplace=True)\n",
    "subs_df[Col.target_nreviews] = TARGET_REVIEWS  \n",
    "subs_df = pd.merge(subs_df, revs_vcounts, on=Col.submission, how='left')\n",
    "subs_df[Col.nreviews] = subs_df[Col.nreviews].fillna(0).astype(int)\n",
    "subs_df[Col.rem_nreviews] = (subs_df[Col.target_nreviews] - subs_df[Col.nreviews]).map(lambda x: max(0, x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "titles = subs_df[Col.title].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keynote - How Are We Managing? Data Teams Management IRL\n",
      "FastAPI and Celery: Building Reliable Web Applications with TDD\n",
      "Apache StreamPipes for Pythonistas: IIoT data handling made easy!\n",
      "Maximizing Efficiency and Scalability in Open-Source MLOps: A Step-by-Step Approach\n",
      "Bringing NLP to Production (an end to end story about some multi-language NLP services)\n",
      "Accelerate Python with Julia\n",
      "Actionable Machine Learning in the Browser with PyScript\n",
      "Common issues with Time Series data and how to solve them\n",
      "The State of Production Machine Learning in 2023\n",
      "Bayesian Marketing Science: Solving Marketing's 3 Biggest Problems\n",
      "Thou Shall Judge But With Fairness: Methods to Ensure an Unbiased Model\n",
      "How Chatbots work – We need to talk!\n",
      "Geospatial Data Processing with Python: A Comprehensive Tutorial\n",
      "BHAD: Explainable unsupervised anomaly detection using Bayesian histograms\n",
      "You've got trust issues, we've got solutions: Differential Privacy\n",
      "Driving down the Memray lane - Profiling your data science work\n",
      "How to connect your application to the world (and avoid sleepless nights)\n",
      "Keynote - Lorem ipsum dolor sit amet\n",
      "How to increase diversity in open source communities\n",
      "Writing Plugin Friendly Python Applications\n",
      "Streamlit meets WebAssembly - stlite\n",
      "WALD: A Modern & Sustainable Analytics Stack\n",
      "Aspect-oriented Programming - Diving deep into Decorators\n",
      "How to baseline in NLP and where to go from there\n",
      "Raised by Pandas, striving for more: An opinionated introduction to Polars\n",
      "Accelerating Python Code\n",
      "“Who is an NLP expert?” - Lessons Learned from building an in-house QA-system\n",
      "Building Hexagonal Python Services\n",
      "Haystack for climate Q/A\n",
      "Introduction to Async programming\n",
      "Neo4j graph databases for climate policy\n",
      "Workshop on Privilege and Ethics in Data\n",
      "Monorepos with Python\n",
      "AutoGluon: AutoML for Tabular, Multimodal and Time Series Data\n",
      "Cloud Infrastructure From Python Code: How Far Could We Go?\n",
      "Building a Personal Assistant With GPT and Haystack: How to Feed Facts to Large Language Models and Reduce Hallucination.\n",
      "Pragmatic ways of using Rust in your data project\n",
      "Accelerating Public Consultations with Large Language Models: A Case Study from the UK Planning Inspectorate\n",
      "Getting started with JAX\n",
      "Machine Learning Lifecycle for NLP Classification in E-Commerce\n",
      "The Beauty of Zarr\n",
      "You are what you read: Building a personal internet front-page with spaCy and Prodigy\n",
      "Rusty Python: A Case Study\n",
      "Staying Alert: How to Implement Continuous Testing for Machine Learning Models\n",
      "Honey, I broke the PyTorch model >.< - Debugging custom PyTorch models in a structured manner\n",
      "The future of the Jupyter Notebook interface\n",
      "Improving Machine Learning from Human Feedback\n",
      "Advanced Visual Search Engine with Self-Supervised Learning (SSL) Representations and Milvus\n",
      "Behind the Scenes of tox: The Journey of Rewriting a Python Tool with more than 10 Million Monthly Downloads\n",
      "The Battle of Giants: Causality vs NLP => From Theory to Practice\n",
      "Visualizing your computer vision data is not a luxury, it's a necessity: without it, your models are blind and so do you.\n",
      "5 Things about fastAPI I wish we had known beforehand\n",
      "How to teach NLP to a newbie & get them started on their first project\n",
      "Unlocking Information - Creating Synthetic Data for Open Access.\n",
      "Create interactive Jupyter websites with JupyterLite\n",
      "Incorporating GPT-3 into practical NLP workflows\n",
      "Teaching Neural Networks a Sense of Geometry\n",
      "Maps with Django\n",
      "What are you yield from?\n",
      "Practical Session: Learning on Heterogeneous Graphs with PyG\n",
      "Introducing FastKafka\n",
      "evosax: JAX-Based Evolution Strategies\n",
      "Using transformers – a drama in 512 tokens\n",
      "Modern typed python: dive into a mature ecosystem from web dev to machine learning\n",
      "Great Security Is One Question Away\n",
      "Pandas 2.0 and beyond\n",
      "Apache Arrow: connecting and accelerating dataframe libraries across the PyData ecosystem\n",
      "Observability for Distributed Computing with Dask\n",
      "MLOps in practice: our journey from batch to real-time inference\n",
      "Data-driven design for the Dask scheduler\n",
      "Exploring the Power of Cyclic Boosting: A Pure-Python, Explainable, and Efficient ML Method\n",
      "The bumps in the road: A retrospective on my data visualisation mistakes\n",
      "Polars - make the switch to lightning-fast dataframes\n",
      "Code Cleanup: A Data Scientist's Guide to Sparkling Code\n",
      "From notebook to pipeline in no time with LineaPy\n",
      "Giving and Receiving Great Feedback through PRs\n",
      "When A/B testing isn’t an option: an introduction to quasi-experimental methods\n",
      "PyLadies Panel Session. Tech Illusions and the Unbalanced Society: Finding Solutions for a Better Future\n",
      "Specifying behavior with Protocols, Typeclasses or Traits. Who wears it better (Python, Scala 3, Rust)?\n",
      "Data Kata: Ensemble programming with Pydantic #1\n",
      "Data Kata: Ensemble programming with Pydantic #2\n",
      "Postmodern Architecture: The Python Powered Modern Data Stack\n",
      "Hyperparameter optimization for the impatient\n",
      "Keynote - Towards Learned Database Systems\n",
      "Keynote - A journey through 4 industries with Python: Python's versatile problem-solving toolkit\n",
      "Most of you don't need Spark. Large-scale data management on a budget with Python\n",
      "The CPU in your browser: WebAssembly demystified\n",
      "Why GPU Clusters Don't Need to Go Brrr? Leverage Compound Sparsity to Achieve the Fastest Inference Performance on CPUs\n",
      "BLE and Python: How to build a simple BLE project on Linux with Python\n",
      "Use Spark from anywhere: A Spark client in Python powered by Spark Connect\n",
      "A concrete guide to time-series databases with Python\n",
      "How to build observability into a ML Platform\n",
      "The Spark of Big Data: An Introduction to Apache Spark\n",
      "Dynamic pricing at Flix\n",
      "Cooking up a ML Platform: Growing pains and lessons learned\n",
      "Contributing to an open-source content library for NLP\n",
      "Delivering AI at Scale\n",
      "How Python enables future computer chips\n",
      "Enabling Machine Learning: How to Optimize Infrastructure, Tools and Teams for ML Workflows\n",
      "Software Design Pattern for Data Science\n",
      "Large Scale Feature Engineering and Datascience with Python & Snowflake\n",
      "Methods for Text Style Transfer: Text Detoxification Case\n",
      "Let's contribute to pandas (3 hours) #1\n",
      "Let's contribute to pandas (3 hours) #2\n",
      "Grokking Anchors: Uncovering What a Machine-Learning Model Relies On\n",
      "Rethinking codes of conduct\n",
      "PyLadies Workshop\n",
      "Fear the mutants. Love the mutants.\n",
      "An unbiased evaluation of environment management and packaging tools\n",
      "Performing Root Cause Analysis with DoWhy, a Causal Machine-Learning Library\n",
      "Have your cake and eat it too: Rapid model development and stable, high-performance deployments\n",
      "Ask-A-Question: an FAQ-answering service for when there's little to no data\n",
      "Prompt Engineering 101: Beginner intro to LangChain, the shovel of our ChatGPT gold rush.\"\n",
      "Shrinking gigabyte sized scikit-learn models for deployment\n",
      "What could possibly go wrong? - An incomplete guide on how to prevent, detect & mitigate biases in data products\n"
     ]
    }
   ],
   "source": [
    "for t in titles:\n",
    "    print(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import wordcloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordcloud.b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save all submission codes for later\n",
    "all_sub_codes = list(subs_df[Col.submission])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bp = sns.barplot(subs_df[Col.nreviews].value_counts().reset_index().rename(columns={'index': '#Reviews/Proposal', Col.nreviews: \"#Proposals\"}), \n",
    "                 x='#Reviews/Proposal', y=\"#Proposals\")\n",
    "bp.set(ylim=(0, len(subs_df.index)));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "progress_df = subs_df.copy()\n",
    "progress_df[Col.nreviews] = progress_df[Col.nreviews].map(lambda x: min(x, TARGET_REVIEWS)) # map more than 3 reviews to 3\n",
    "progress_df = progress_df[[Col.target_nreviews, Col.nreviews]].sum().to_frame().T\n",
    "f, ax = plt.subplots(figsize=(15, 1))\n",
    "sns.set_color_codes(\"pastel\")\n",
    "sns.barplot(data=subs_df[[Col.target_nreviews, Col.nreviews]].sum().to_frame().T, x=Col.target_nreviews, color='b')\n",
    "sns.set_color_codes(\"muted\")\n",
    "ax = sns.barplot(data=progress_df, x=Col.nreviews, color='b');\n",
    "ax.bar_label(ax.containers[1], labels=[\"{:.1%}\".format(progress_df.loc[0, Col.nreviews] / progress_df.loc[0, Col.target_nreviews])]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Get spreadsheat with reviewers and preferences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gsheet_client = GSheetClient()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gsheet_df = gsheet_client.gsheet_as_df(cfg['spreadsheet_id'], cfg['worksheet_name'])\n",
    "# rename columns to stick to our convention\n",
    "col_map = {\n",
    " \"Topics you want to review\": Col.track_prefs,\n",
    " \"Email address\": Col.email,\n",
    " \"Name\": Col.speaker_name,\n",
    " \"Affiliation\": Col.affiliation,\n",
    " \"Who do you know from the Committee?\": Col.committee_contact,\n",
    " \"Availability during the Review Period\": Col.availability,\n",
    " \"Additional comments regarding your availability during the review period.\": Col.availability_comment,\n",
    " \"Activated in Pretalx\": Col.pretalx_activated,\n",
    " \"Do you want your name to be listed as a reviewer on the conference website?\": Col.public,\n",
    " \"Wants all proposals\": Col.all_proposals,\n",
    " \"Any additional comments for the Program Committee\": Col.comment,\n",
    " \"Pretalx Name\": Col.pretalx_user,\n",
    "}\n",
    "gsheet_df.rename(columns=col_map, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Do some transformations to handle the GSheet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parse preferences\n",
    "gsheet_df[Col.track_prefs] = gsheet_df[Col.track_prefs].apply(lambda x: x.replace(community_map[0], community_map[1]).split(', '))\n",
    "gsheet_df = gsheet_df.loc[~gsheet_df[Col.pretalx_activated].isna()]\n",
    "# save people that want all proposals for later\n",
    "assign_all_emails = gsheet_df[Col.email].loc[gsheet_df[Col.all_proposals] == 'x'].tolist()\n",
    "gsheet_df = gsheet_df.loc[gsheet_df[Col.all_proposals] != 'x']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assign_df = pd.merge(gsheet_df, revs_df, on=Col.pretalx_user, how='left')\n",
    "assign_df[\"review\"] = assign_df[\"review\"].apply(lambda x: x if isinstance(x, list) else [])\n",
    "assign_df[Col.curr_assignments] = assign_df[\"review\"].map(lambda x: x[:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assign_df = assign_df.assign(**{Col.done_nreviews: assign_df[\"score\"].map(\n",
    "    lambda scores: 0 if not isinstance(scores, list) else len([s for s in scores if not np.isnan(s)]))}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.barplot(assign_df[Col.done_nreviews].value_counts().reset_index().rename(\n",
    "    columns={'index': Col.done_nreviews, Col.done_nreviews: \"#Reviewers\"}), \n",
    "    x=Col.done_nreviews, \n",
    "    y=\"#Reviewers\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "active_rev = pd.DataFrame({\n",
    "    \"Active Reviewers\": [assign_df.assign(started=assign_df[Col.done_nreviews] > 0).groupby(\"started\").count()[Col.speaker_name].loc[True]],\n",
    "    \"all\": [len(assign_df)]\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f, ax = plt.subplots(figsize=(15, 1))\n",
    "sns.set_color_codes(\"pastel\")\n",
    "sns.barplot(data=active_rev, x=\"all\", color='g')\n",
    "sns.set_color_codes(\"muted\")\n",
    "ax = sns.barplot(data=active_rev, x=\"Active Reviewers\", color='g');\n",
    "ax.bar_label(ax.containers[1], labels=[\"{:.1%}\".format(active_rev.loc[0, \"Active Reviewers\"] / active_rev.loc[0, \"all\"])]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assign remaining proposals to reviewers\n",
    "\n",
    "The main idea is to assign each the number of needed reviews plus a buffer for a proposal/submission: \n",
    "* not a person having already assigned the review for a submission (no duplicates)\n",
    "* to a person having a preference for the track with the least amount of current work.\n",
    "* if no person has a preference for the track of the proposal, assign to someone with not much work.\n",
    "\n",
    "Since we will run this dynamically the moment the needed number is reached, the proposal will be deallocated from people that were assigned the buffer.\n",
    "\n",
    "(it might be that someone gets by accident assigned his/her own proposal but Pretalx takes care of that if the same user e-mail was used)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_user(df, is_preference, is_already_assigned):\n",
    "    mask = is_preference & ~is_already_assigned\n",
    "    if df.loc[mask].empty:\n",
    "        return df.loc[~is_already_assigned, \"New Assignments\"].idxmin()\n",
    "    else:\n",
    "        return df.loc[mask, \"New Assignments\"].idxmin()\n",
    "\n",
    "def assign_proposals(dist_df, curr_df, buffer=3):\n",
    "    dist_df = dist_df.sort_values(Col.rem_nreviews, ascending=False)\n",
    "    curr_df = curr_df.copy()\n",
    "    for _, row in dist_df.iterrows():\n",
    "        is_preference = curr_df[Col.track_prefs].map(lambda x: row[Col.track] in x)\n",
    "        is_already_assigned = curr_df[Col.curr_assignments].map(lambda x: row[Col.submission] in x)\n",
    "        if row[Col.rem_nreviews] < 1:\n",
    "            continue\n",
    "        for _ in range(row[Col.rem_nreviews] + buffer):\n",
    "            idx = find_user(curr_df, is_preference, is_already_assigned)\n",
    "            curr_df.loc[idx, Col.curr_assignments].append(row[Col.submission])\n",
    "            curr_df.loc[idx, \"New Assignments\"] += 1\n",
    "    return curr_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#assign_df[\"New Assignments\"] = assign_df[Col.curr_assignments].map(len) # keep track of what was done so far\n",
    "#start giving everyone some more things to do\n",
    "assign_df[\"New Assignments\"] = 0\n",
    "\n",
    "assign_df = assign_df.sample(frac=1, random_state=RND_STATE).reset_index(drop=True)\n",
    "new_assign_df = assign_proposals(subs_df, assign_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add people that want all proposals assigned again\n",
    "all_subs_df = pd.DataFrame({Col.email: assign_all_emails, Col.curr_assignments: [all_sub_codes] * len(assign_all_emails)})\n",
    "new_assign_df = pd.concat([new_assign_df, all_subs_df]).reset_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save it as JSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_assignments_as_json(new_assign_df, \"assignments_20230131_1.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
